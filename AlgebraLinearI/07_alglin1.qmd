--- 
title: "O posto de uma matriz"
number-sections: true
lang: pt-BR
--- 

Vamos considerar mais um invariante, essa vez para qualquer matriz
retangular $A$: o seu posto $\tn{Rk}(A)$. Como acima, no caso especial
em que $A,B$ são quadradas e semelhantes, vamos ter
$\tn{Rk}(A) = \tn{Rk}(B)$.

:::{#exm-prod-mat}
Nesta seção, vamos precisar usar algumas observações sobre multiplicação matricial. Assuma que $X$, $Y$, $Z$ são matrizes $m\times n$, $m\times k$, $k\times n$, respetivamente, tais que 
$$ 
  X=YZ.
$$ 
Isso quer dizer que 
$$ 
X_{i,j}=\sum_{a=1}^k Y_{i,a}Z_{a,j}.
$$
Fixando $j$ e deixando $i$ variar entre $1$ e $m$, obtemos que 
$$
\begin{pmatrix}
X_{1,j}\\ \vdots\\ X_{m,j}\end{pmatrix} = 
\sum_{a=1}^k Z_{a,j}\begin{pmatrix} Y_{1,a}\\ \vdots\\ Y_{m,a}\end{pmatrix}. 
$$  
Em outras palavras, a $j$-ésima coluna de $X$ é uma combinação linear das colunas de $Y$ e os coeficientes são as entradas na $j$-ésima coluna de $Z$. Então, denotando as colunas de $X$ por $C_1,\ldots,C_n$, temos que 
$$
\begin{pmatrix} C_1 & \cdots & C_n\end{pmatrix}=\begin{pmatrix} Y_1 & \cdots & Y_k\end{pmatrix}Z 
$$
onde $Y_1,\ldots,Y_k$ são as colunas de $Y$. 

Por argumento similar, se $L_1,\ldots,L_m$ são as linhas de $X$ e $Z_1,\ldots,Z_k$ são as linhas de $Z$, obtemos que 
$$
\begin{pmatrix} L_1 \\ \vdots \\ L_m\end{pmatrix} = Y\begin{pmatrix} Z_1 \\ \vdots \\ Z_k\end{pmatrix}
$$
e a $i$-ésima linha de $X$ é combinação linear das linhas de $Z$ com coeficientes que são entradas na $i$-ésima linha de $Y$.
:::

:::{#def-posto}
Seja $A$ uma matriz $m\times n$.

-   O *espaço coluna* $\tn{Col}(A)$ de $A$ é o subespaço de $\R^m$
    gerado pelas colunas de $A$.

-   O *espaço linha* $\tn{Lin}(A)$ de $A$ é o subespaço de $\R^n$ gerado
    pelas linhas de $A$.
:::

Da @def-posto, e do @exm-prod-mat obtemos o seguinte resultado.

:::{#lem-posto-prod}
Assuma que $X$, $Y$, $Z$ são matrizes $m\times n$, $m\times k$, $k\times n$, respetivamente, tais que 
$$ 
  X=YZ.
$$
Então 
$$
\dim\tn{Col}(X)\leq \dim\tn{Col}(Y)\leq k
$$
e
$$
\dim\tn{Lin}(X)\leq \dim\tn{Lin}(Z)\leq k
$$
:::

:::{#exm-}
Seja $$A = \begin{pmatrix}
1 & 1 & 2 \\
2 & 1 & 3 \\
0 & 1 & 1 \\
1 & 0 & 1
\end{pmatrix}.$$

-   $\tn{Col}(A)$ é o subespaço de $\R^4$ gerado pelo conjunto
    $$X = \left\{\begin{pmatrix}
      1 \\ 2 \\ 0 \\ 1
      \end{pmatrix}\,,\, \begin{pmatrix}
      1 \\ 1 \\ 1 \\ 0
      \end{pmatrix}\,,\,\begin{pmatrix}
      2 \\ 3 \\ 1 \\ 1
      \end{pmatrix} \right\}.$$ $\tn{Col}(A)$ claramente tem dimensão
    pelo menos duas. Escalonando a matriz $$\begin{pmatrix}
    1 & 1 & 2 & 0\\
    2 & 1 & 3 & 0\\
    0 & 1 & 1 & 0\\
    1 & 0 & 1 & 0
    \end{pmatrix}\equiv \begin{pmatrix}
    1 & 1 & 2 & 0\\
    0 & -1 & -1 & 0\\
    0 & 1 & 1 & 0\\
    0 & -1 & -1 & 0
    \end{pmatrix}\equiv \begin{pmatrix}
    1 & 0 & 1 & 0\\
    0 & 1 & 1 & 0\\
    0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0
    \end{pmatrix},$$ vimos que a terceira coluna não possui pivô, então
    existem soluções não triviais da equação $$\lambda_1\begin{pmatrix}
      1 \\ 2 \\ 0 \\ 1
      \end{pmatrix} + \lambda_2\begin{pmatrix}
      1 \\ 1 \\ 1 \\ 0
      \end{pmatrix} + \lambda_3\begin{pmatrix}
      2 \\ 3 \\ 1 \\ 1
      \end{pmatrix} = \ul{0}.$$ Já que $X$ é LD, $\tn{Col}(A)$ tem
    dimensão 2.

-   $\tn{Lin}(A)$ é o subespaço de $\R^3$ gerado pelo conjunto
    $$Y = \left\{\begin{pmatrix}
      1 \\ 1 \\ 2
      \end{pmatrix}\,,\, \begin{pmatrix}
      2 \\ 1 \\ 3
      \end{pmatrix}\,,\,\begin{pmatrix}
      0 \\ 1 \\ 1
      \end{pmatrix}\,,\,\begin{pmatrix}
      1 \\ 0 \\ 1
      \end{pmatrix} \right\}.$$ Escalonando: $$\begin{pmatrix}
    1 & 2 & 0 & 1 & 0\\
    1 & 1 & 1 & 0 & 0\\
    2 & 3 & 1 & 1 & 0
    \end{pmatrix}\equiv \begin{pmatrix}
    1 & 2 & 0 & 1 & 0\\
    0 & -1 & 1 & -1 & 0\\
    0 & -1 & 1 & -1 & 0
    \end{pmatrix}\equiv \begin{pmatrix}
    1 & 0 & 2 & -1 & 0\\
    0 & 1 & -1 & 1 & 0\\
    0 & 0 & 0 & 0 & 0
    \end{pmatrix}$$ vimos que $\begin{pmatrix}
      1 \\ 1 \\ 2
      \end{pmatrix}\,,\, \begin{pmatrix}
      2 \\ 1 \\ 3
      \end{pmatrix}$ são combinações lineares de $\begin{pmatrix}
      0 \\ 1 \\ 1
      \end{pmatrix}\,,\,\begin{pmatrix}
      1 \\ 0 \\ 1
      \end{pmatrix}$. Logo $\tn{dim}(\tn{Lin}(A)) = 2$ também.
:::

De fato:

:::{#thm-}
Para qualquer matriz $m\times n$ $A$,
$$\dim(\tn{Col}(A)) = \dim(\tn{Lin}(A)).$$
:::

:::{.proof}
Seja 
$$
A=\begin{pmatrix} 
A_1 & \cdots & A_n
\end{pmatrix},$$ com os $A_i$ sendo as colunas de $A$.

Suponha que $\tn{dim}(\tn{Col}(A)) = r$ e seja
$X = \{\ul{c_1}\,,\,\ldots\,,\,\ul{c_r}\}$ uma base de $\tn{Col}(A)$.
Vamos mostrar que $\dim(\tn{Lin}(A))\leqslant r$. Considere a matriz
$m\times r$ $$C = \begin{pmatrix}
\ul{c_1} & \cdots & \ul{c_r}
\end{pmatrix},$$ com colunas os vetores da base $X$ de $\tn{Col}(A)$. Já
que $X$ é uma base de $\tn{Col}(A)$, toda coluna $A_i$ pode ser escrita
como combinação linear dos $\ul{c_1},\ldots,\ul{c_r}$:
$$A_i = s_{1i}\ul{c_1} + \cdots + s_{ri}\ul{c_r}.$$ Sendo $S$ a matriz
$r\times n$ $$S = \begin{pmatrix}
s_{11} & \cdots &s_{1n} \\
\vdots & & \vdots \\
s_{r1} & \cdots & s_{rn}
\end{pmatrix},$$ temos que $A = CS$ (confirme!). 
Usando @lem-posto-prod, obtemos que 
$$
\dim\tn{Lin}(A)\leq \dim\tn{Lin}(S)\leq r=\dim\tn{Col}(A).
$$
<!--Vamos reinterpretar
essa equação. Escrevendo $$A = \begin{pmatrix}
B_1 \\ \vdots \\ B_m
\end{pmatrix}$$ com os $B_i$ as linhas de $A$ e $$S = \begin{pmatrix}
S_1 \\ \vdots \\ S_r
\end{pmatrix}$$ com os $S_i$ as linhas de $S$, a equação $A = CS$ se
torna $$\begin{pmatrix}
B_1 \\ \vdots \\ B_m
\end{pmatrix} = C\begin{pmatrix}
S_1 \\ \vdots \\ S_r
\end{pmatrix},$$ mostrando que cada linha de $A$ é combinação linear das
linhas de $S$. Ou seja, $\tn{Lin}(A)$ é gerado pelas $r$ linhas de $S$.
Ou seja, $\dim(\tn{Lin}(A))\leqslant r = \dim(\tn{Col}(A))$. -->

Mas agora, aplicando o mesmo argumento pra matriz transposta $A^t$,
obtemos que
$$\dim(\tn{Col}(A)) = \dim(\tn{Lin}(A^t)) \leqslant \dim(\tn{Col}(A^t)) = \dim(\tn{Lin}(A)),$$
logo $\dim(\tn{Col}(A)) = \dim(\tn{Lin}(A))$. ◻
:::

:::{#def-}
O *posto* $\tn{Rk}(A)$ de uma matriz $A$ é a dimensão do espaço coluna
(ou do espaço linha) de $A$.

(as letras $\tn{Rk}$ vem do nome inglés *rank*).
:::

:::{#exm-}
Seja $$A = \begin{pmatrix}
1 & 1 & 2 \\
2 & 1 & 3 \\
0 & 1 & 1 \\
1 & 0 & 1
\end{pmatrix}.$$ Já vimos que
${\rm dim}({\rm Col}(A) ) = {\rm dim}({\rm Lin}(A)) = 2$, portanto
${\rm Rk}(A) = 2$.
:::

:::{#exm-}
Sejam $B = \{(1,1)\,,\,(0,2) \}$ e
$B' = \{(1,0,1)\,,\,(0,1,0)\,,\,(1,2,0) \}$ bases de $\mathbb{R}^2$ e
$\mathbb{R}^3$, respetivamente e
$T: \mathbb{R}^2 \rightarrow \mathbb{R}^3$ a transformação linear
definida por $(x,y) \mapsto (2x,x-y,2y)$.

Sabemos que $$[T]^{B}_{B'} =  \begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
a_{31} & a_{32} 
\end{pmatrix},$$ em que $T(1,1)_{B'} = \begin{pmatrix}
a_{11}  \\
a_{21}  \\
a_{31} 
\end{pmatrix}$ e $T(0,2)_{B'} = \begin{pmatrix}
a_{12} \\
a_{22} \\
a_{32} 
\end{pmatrix}$. Temos
$$T(1,1) = (2,0,2) = a_{11}(1,0,1)+ a_{21}(0,1,0)+ a_{31}(1,2,0);$$
$$T(0,2) = (0,-2,4) = a_{12}(1,0,1)+ a_{22}(0,1,0)+ a_{32}(1,2,0);$$
resolvendo os sistemas, obtemos $$[T]^{B}_{B'} =
    \begin{pmatrix}
    2 & 4 \\ 
    0& 6 \\
    0 & -4
\end{pmatrix}.$$ Note que ${\rm dim}({\rm Col}([T]^{B}_{B'})) = 2$, logo
${\rm Rk}([T]^{B}_{B'}) = 2$.

Como $$\begin{array}{ccc}
(2x,x-y,2y) &= & (2x,x,0) + (0,-y,2y) \\
&=& x(2,1,0) + y(0,-1,2)
\end{array},$$ segue que $\{ (2,1,0), (0,-1,2)\}$ é uma base para
${\rm Im}(T)$, logo
${\rm dim(}{\rm Im}(T)) = 2 = {\rm Rk}([T]^{B}_{B'})$.
:::

De fato, esta igualdade sempre vale:

:::{#thm-}
Sejam $V$ e $W$ espaços vetoriais de dimensão finita,
$T: V \rightarrow W$ uma transformação linear e
$B = \{\ul{b_1},\ldots,\ul{b_m}\}$ e $C$ bases de $V$ e $W$,
respetivamente. Então
$${\rm dim}({\rm Im}(T)) = {\rm Rk}([T]^{B}_{C}).$$
:::

:::{.proof}

Se lembre que as colunas de $[T]_C^B$ são os vetores
$$\{T(\ul{b_1})_C,\ldots,T(\ul{b_m})_C\}.$$ Logo
$$\dim(\tn{Col}( [T]_C^B)) = \dim\left(\left<T(\ul{b_1})_C,\ldots,T(\ul{b_m})_C\right>\right)\leqslant \dim(\tn{Im}(T)).$$
Mas na outra direção, um vetor $\ul{w}$ da imagem de $T$ tem a forma
$T(\ul{v})$. Escrevendo
$\ul{v} = \lambda_1\ul{b_1}+\cdots+\lambda_m\ul{b_m}$, o vetor de
coordenadas de $\ul{v}$ na base $B$ é $\ul{v}_B = \begin{pmatrix}
\lambda_1 \\
\vdots \\
\lambda_m 
\end{pmatrix}$ e
$$\ul{w}_C = T(\ul{v})_C = \lambda_1 T(\ul{b_1})_C + \ldots + \lambda_m T(\ul{b_m})_C.$$
Segue que $\ul{w}_C\in \tn{Col}([T]_C^B)$, logo
$$\dim(\tn{Im}(T))\leqslant \dim(\tn{Col}(T^B_C)).$$ Resumindo,
$$\dim(\tn{Im}(T)) = \dim(\tn{Col}([T^B_C])) = \tn{Rk}([T]_C^B).$$ ◻
:::

Se lembre que mostramos na última lista que as matrizes $n\times n$
$A,B$ são semelhantes se, e somente se, existem um espaço vetorial $V$
de dimensão $n$ e um endomorfismo $T:V\to V$ tais que
$$[T]_X^X = A\,,\, [T]_Y^Y = B$$ em que $X,Y$ são duas bases de $V$.

:::{#cor-}
Sejam $A$ e $B$ matrizes semelhantes. Então
$${\rm Rk}(A) = {\rm Rk}(B).$$
:::

:::{.proof}

as matrizes de $T$ com respeito a duas bases de $V$. Então pelo teorema,
$$\tn{Rk}(A) = \dim(\tn{Im}(T)) = \tn{Rk}(B).$$ ◻
:::
